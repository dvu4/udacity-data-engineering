import configparser


# CONFIG
# HOST variable in the dwh.cfg is the web address to your redshift cluster  (i.e Endpoint)
# ARN variable in the dwh.cfg is IAM Role arn
config = configparser.ConfigParser()
config.read('dwh.cfg')

# DROP TABLES

staging_events_table_drop = "DROP TABLE IF EXISTS staging_events"
staging_songs_table_drop  = "DROP TABLE IF EXISTS staging_songs"
songplay_table_drop       = "DROP TABLE IF EXISTS fact_songplay"
user_table_drop           = "DROP TABLE IF EXISTS dim_user"
song_table_drop           = "DROP TABLE IF EXISTS dim_song"
artist_table_drop         = "DROP TABLE IF EXISTS dim_artist"
time_table_drop           = "DROP TABLE IF EXISTS dim_time"


# CREATE STAGING TABLES

# Staging table is a temporary table that is used to stage the data for temporary purpose just before loading it to the Target table from the Source Table.
# When the volume or granularity of the transformation process causes ETL processes to perform poorly, we consider using a staging table on the destination database as a vehicle for processing interim data results. 
# Staging tables are normally considered volatile tables, meaning that they are emptied and reloaded each time without persisting the results from one execution to the next. Staging tables should be used only for interim results and not for permanent storage.
# log_data is the second dataset consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. 
# These simulate activity logs from a music streaming app based on specified configurations.

# 2018-11-01-events.json file looks like : 
#{
#    "artist":"Barry Tuckwell\/Academy of St Martin-in-the-Fields\/Sir Neville Marriner",
#    "auth":"Logged In",
#    "firstName":"Celeste",
#    "gender":"F",
#    "itemInSession":1,
#    "lastName":"Williams",
#    "length":277.15873,
#    "level":"free",
#    "location":"Klamath Falls, OR",
#    "method":"PUT",
#    "page":"NextSong",
#    "registration":1541077528796.0,
#    "sessionId":438,"song":"Horn Concerto No. 4 in E flat K495: II. Romance (Andante cantabile)",
#    "status":200,
#    "ts":1541990264796,
#    "userAgent":"\"Mozilla\/5.0 (Windows NT 6.1; WOW64) AppleWebKit\/537.36 (KHTML, like Gecko) Chrome\/37.0.2062.103 Safari\/537.36\"",
#    "userId":"53"
#}    
staging_events_table_create= ("""
CREATE TABLE IF NOT EXISTS staging_events 
(
        artist        VARCHAR,
        auth          VARCHAR,
        firstName     VARCHAR,
        gender        VARCHAR,
        itemInSession INTEGER,
        lastName      VARCHAR,
        length        FLOAT, 
        level         VARCHAR,
        location      VARCHAR,
        method        VARCHAR,
        page          VARCHAR,
        registration  FLOAT, 
        sessionId     INTEGER,
        song          VARCHAR,
        status        INTEGER,
        ts            TIMESTAMP,
        userAgent     VARCHAR,
        userId        INTEGER
);
""")


# song_data is a subset of real data from the Million Song Dataset. 
# Each file is in JSON format and contains metadata about a song and the artist of that song. 
# The files are partitioned by the first three letters of each song's track ID.

# TRAAAAW128F429D538.json file looks like : 
#    "num_songs": 1, 
#    "artist_id": "ARD7TVE1187B99BFB1", 
#    "artist_latitude": null, 
#    "artist_longitude": null, 
#    "artist_location": "California - LA", 
#    "artist_name": "Casual", 
#    "song_id": "SOMZWCG12A8C13C480", 
#    "title": "I Didn't Mean To", 
#    "duration": 218.93179, 
#    "year": 0

staging_songs_table_create = ("""
CREATE TABLE IF NOT EXISTS staging_songs
(
        song_id           VARCHAR PRIMARY KEY,
        artist_id         VARCHAR,
        artist_latitude   FLOAT, 
        artist_location   VARCHAR, 
        artist_longitude  FLOAT, 
        artist_name       VARCHAR
        duration          FLOAT, 
        num_songs         INTEGER,
        title             VARCHAR,
        year              INTEGER
);
""")


    
# CREATE TABLES

# songplay_id SERIAL PRIMARY KEY --> songplay_id INTEGER IDENTITY(0,1) PRIMARY KEY  
songplay_table_create = ("""
CREATE TABLE IF NOT EXISTS fact_songplay 
(
            songplay_id INTEGER IDENTITY(0,1) PRIMARY KEY sortkey, 
            start_time  TIMESTAMP NOT NULL REFERENCES dim_time(start_time), 
            user_id     INTEGER NOT NULL REFERENCES dim_user(user_id), 
            level       VARCHAR,
            song_id     VARCHAR NOT NULL REFERENCES dim_song(song_id), 
            artist_id   VARCHAR NOT NULL REFERENCES dim_artist(artist_id), 
            session_id  INTEGER, 
            location    VARCHAR, 
            user_agent  VARCHAR,
);
""")


user_table_create = ("""
CREATE TABLE IF NOT EXISTS dim_user 
(
            user_id    INTEGER PRIMARY KEY distkey, 
            first_name VARCHAR, 
            last_name  VARCHAR, 
            gender     VARCHAR, 
            level      VARCHAR
);
""")

song_table_create = ("""
CREATE TABLE IF NOT EXISTS dim_song 
(
            song_id   VARCHAR PRIMARY KEY sortkey, 
            title     VARCHAR, 
            artist_id VARCHAR, 
            year      INTEGER, 
            duration  FLOAT
);
""")

artist_table_create = ("""
CREATE TABLE IF NOT EXISTS dim_artist 
(
            artist_id VARCHAR PRIMARY KEY distkey, 
            name      VARCHAR, 
            location  VARCHAR, 
            latitude  FLOAT, 
            longitude FLOAT
);
""")

time_table_create = ("""
CREATE TABLE IF NOT EXISTS dim_time (
            start_time TIMESTAMP PRIMARY KEY sortkey distkey, 
            hour       INTEGER, 
            day        INTEGER, 
            week       INTEGER, 
            month      INTEGER, 
            year       INTEGER, 
            weekday    INTEGER
);
""")


# STAGING TABLES
staging_events_copy = """
COPY staging_events 
FROM {}
CREDENTIALS 'aws_iam_role={}'
COMPUPDATE OFF region 'us-west-2'
FORMAT AS json {}
""".format(config.get('S3',       'LOG_DATA'),
           config.get('IAM_ROLE', 'ARN'),
           config.get('S3',       'LOG_JSONPATH')
          )


staging_songs_copy = """
COPY staging_songs 
FROM {}
CREDENTIALS 'aws_iam_role={}'
COMPUPDATE OFF region 'us-west-2'
FORMAT AS json 'auto'
""".format(config.get('S3',       'SONG_DATA'), 
           config.get('IAM_ROLE', 'ARN')
          )




# FINAL TABLES
 
##############################
# TO_TIMESTAMP(TO_CHAR(se.ts, , '9999-99-99 99:99:99'),'YYYY-MM-DD HH24:MI:SS') AS start_time           
songplay_table_insert = (""" 
INSERT INTO fact_songplay (start_time, user_id, level, song_id, artist_id, session_id, location, user_agent)
SELECT DISTINCT TO_TIMESTAMP(se.ts,'YYYY-MM-DD HH24:MI:SS') AS start_time
                se.userId    AS user_id, 
                se.level     AS level, 
                ss.song_id   AS song_id, 
                ss.artist_id AS artist_id, 
                se.sessionId AS session_id, 
                se.location  AS location, 
                se.userAgent AS user_agent 
FROM staging_songs ss
JOIN staging_events se 
ON (ss.title = se.song AND ss.artist_name = se.artist)
AND se.page = 'NextSong';
""")
           
# 
##############################
user_table_insert = ("""
INSERT INTO dim_user (user_id, first_name, last_name, gender, level)
SELECT DINSTICT userId    AS user_id,
                firstName AS first_name, 
                lastName  AS last_name, 
                gender    AS gender, 
                level     AS level
FROM staging_events
WHERE userId IS NOT NULL
AND page = 'NextSong';
""")

           
##############################    
song_table_insert = ("""
INSERT INTO dim_song (song_id, title, artist_id, year, duration)
SELECT DISTINCT song_id AS song_id,
                title AS title, 
                artist_id AS artist_id, 
                year AS year, 
                duration AS duration
FROM  staging_songs
WHERE song_id IS NOT NULL;   
""")

    
##############################         
artist_table_insert = ("""
INSERT INTO dim_artist (artist_id, name, location, latitude, longitude)
SELECT DISTINCT artist_id AS artist_id, 
                artist_name AS name, 
                artist_location AS location,  
                artist_latitude AS latitude, 
                artist_longitude AS longitude
FROM  staging_songs
WHERE artist_id IS NOT NULL; 
""")

           
##############################         
# TO_TIMESTAMP(TO_CHAR(se.ts, , '9999-99-99 99:99:99'),'YYYY-MM-DD HH24:MI:SS') AS start_time
time_table_insert = ("""
INSERT INTO dim_time (start_time, hour, day, week, month, year, weekday)
SELECT DISTINCT TO_TIMESTAMP(ts,'YYYY-MM-DD HH24:MI:SS') AS start_time,
                EXTRACT(HOUR    FROM ts) AS hour,
                EXTRACT(DAY     FROM ts) AS day, 
                EXTRACT(WEEK    FROM ts) AS week, 
                EXTRACT(MONTH   FROM ts) AS month, 
                EXTRACT(YEAR    FROM ts) AS year,
                EXTRACT(WEEKDAY FROM ts) AS  weekday
FROM staging_events 
WHERE ts IS NOT NULL;
""")

# QUERY LISTS

create_table_queries = [staging_events_table_create, staging_songs_table_create, songplay_table_create, user_table_create, song_table_create, artist_table_create, time_table_create]
drop_table_queries = [staging_events_table_drop, staging_songs_table_drop, songplay_table_drop, user_table_drop, song_table_drop, artist_table_drop, time_table_drop]
copy_table_queries = [staging_events_copy, staging_songs_copy]
insert_table_queries = [songplay_table_insert, user_table_insert, song_table_insert, artist_table_insert, time_table_insert]
